{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mG2Q7KWb5Dmv",
    "outputId": "b6624567-c479-46cc-907e-8c49bda901e6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow==2.10.1\n",
      "  Downloading tensorflow-2.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 578.1 MB 7.1 kB/s \n",
      "\u001B[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001B[K     |████████████████████████████████| 438 kB 77.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (4.4.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (14.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (1.15.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (2.1.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (3.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (0.28.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (1.51.1)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.7 MB 58.6 MB/s \n",
      "\u001B[?25hCollecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "\u001B[K     |████████████████████████████████| 5.9 MB 71.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (57.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (1.21.6)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.10.1) (3.19.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.1) (0.38.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.15.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.23.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (5.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (2022.12.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.1) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow==2.10.1) (3.0.9)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, keras, flatbuffers, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.9.0\n",
      "    Uninstalling keras-2.9.0:\n",
      "      Successfully uninstalled keras-2.9.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.9.2\n",
      "    Uninstalling tensorflow-2.9.2:\n",
      "      Successfully uninstalled tensorflow-2.9.2\n",
      "Successfully installed flatbuffers-22.12.6 keras-2.10.0 tensorboard-2.10.1 tensorflow-2.10.1 tensorflow-estimator-2.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64eIEDRsEewT",
    "outputId": "a2d66bfb-2aec-4902-ab94-9c12b45ab828"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting focal-loss\n",
      "  Downloading focal_loss-0.0.7-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.8/dist-packages (from focal-loss) (2.10.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (22.12.6)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (3.19.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.21.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (21.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (3.1.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (0.28.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.51.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (14.0.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (57.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (4.4.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (2.10.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (1.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.2->focal-loss) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2->focal-loss) (0.38.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (2.23.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (2.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (5.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (2022.12.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow>=2.2->focal-loss) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow>=2.2->focal-loss) (3.0.9)\n",
      "Installing collected packages: focal-loss\n",
      "Successfully installed focal-loss-0.0.7\n"
     ]
    }
   ],
   "source": [
    "pip install focal-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GDrive connection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtPN1GPqhk2h",
    "outputId": "cec951f4-baaf-43ad-bf3e-d376ec3f37cf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LuTPiZoiDMo",
    "outputId": "fc711d95-d264-405f-f5ae-8358d096da90"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/gdrive/.shortcut-targets-by-id/12JPPVxLkWQkx591Su9530FzjA6jNjEnR/HMWK2\n"
     ]
    }
   ],
   "source": [
    "%cd /gdrive/My Drive/HMWK2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXNePTG8pm5V"
   },
   "outputs": [],
   "source": [
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16) \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import interp1d\n",
    "from numpy.random import default_rng\n",
    "from sklearn.utils import class_weight\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Env Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWoltGVmsVAZ"
   },
   "outputs": [],
   "source": [
    "# Fixed random seed to make results as reproducible as possible\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3_eckx3iwmC"
   },
   "source": [
    "# Data loading - preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions for time series 2 image conversion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function is used to normalize an array between 0 and 255 using NumPy library. Here's how it works:\n",
    "\n",
    "The function takes an input array as an argument.\n",
    "- For each row (i.e., first dimension) of the input array, the function flattens it to a 1D array using the flatten() method.\n",
    "- The interp() function from NumPy is used to map the values from their original range (determined by flattened.min() and flattened.max()) to the desired range of 0 to 255.\n",
    "- The reshape() method is used to transform the 1D array back into its original shape.\n",
    "- The normalized row is then assigned back to its original position in the input array.\n",
    "- The normalized array is returned as the output.\n",
    "\n",
    "In this project it is used to map a time series into 0-255 values to resemble 8-bit pixel values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3AWq3xxpLFd"
   },
   "outputs": [],
   "source": [
    "def shifter0255(array):\n",
    "  for i in range(array.shape[0]):\n",
    "    flattened = array[i].flatten()\n",
    "    shifted = np.interp(flattened, (flattened.min(), flattened.max()), (0, 255))\n",
    "    array[i] = np.reshape(shifted, array[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function is used to transform a time series data into an image format using NumPy library. Here's how it works:\n",
    "\n",
    "The function takes a time series of size 36x6 as input, which is represented as a 2D NumPy array x.\n",
    "- The input array is then repeated six times along the last axis using the np.repeat() function with the repeats parameter set to 6, resulting in a 2D NumPy array x with shape (36, 36).\n",
    "- The shifter0255() function is then applied to the x array to normalize the values between 0 and 255.\n",
    "- The normalized x array is then stacked into a 3D NumPy array x with shape (36, 36, 3) using the np.stack() function, where the last dimension is set to 3 to create an RGB image by copying the normalized array to each of the color channels.\n",
    "- The resulting 3D array x is returned as the output.\n",
    "\n",
    "- Overall, this function can be useful for visualizing time series data in an image format, where each row of the image represents a time step and the columns represent different features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzcWPbho-Wl-"
   },
   "outputs": [],
   "source": [
    "def timeSeriesToImage(x):\n",
    "  x = np.repeat(x, repeats = 6, axis = -1)\n",
    "  shifter0255(x)\n",
    "  x = np.stack([x, x, x], axis = -1)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time series augmentations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAP-06xd6r6f"
   },
   "outputs": [],
   "source": [
    "def jitter(x, sigma=0.03):\n",
    "    # https://arxiv.org/pdf/1706.00527.pdf\n",
    "    return x + np.random.normal(loc=0., scale=sigma, size=x.shape)\n",
    "\n",
    "def scaling(x, sigma=0.1):\n",
    "    # https://arxiv.org/pdf/1706.00527.pdf\n",
    "    factor = np.random.normal(loc=1., scale=sigma, size=(x.shape[0],x.shape[2]))\n",
    "    return np.multiply(x, factor[:,np.newaxis,:])\n",
    "\n",
    "\n",
    "def time_warp(x, sigma=0.2, knot=4):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    orig_steps = np.arange(x.shape[1])\n",
    "    \n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n",
    "    warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n",
    "    \n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        for dim in range(x.shape[2]):\n",
    "            time_warp = CubicSpline(warp_steps[:,dim], warp_steps[:,dim] * random_warps[i,:,dim])(orig_steps)\n",
    "            scale = (x.shape[1]-1)/time_warp[-1]\n",
    "            ret[i,:,dim] = np.interp(orig_steps, np.clip(scale*time_warp, 0, x.shape[1]-1), pat[:,dim]).T\n",
    "    return ret\n",
    "\n",
    "\n",
    "def window_slice(x, reduce_ratio=0.9):\n",
    "    # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n",
    "    target_len = np.ceil(reduce_ratio*x.shape[1]).astype(int)\n",
    "    if target_len >= x.shape[1]:\n",
    "        return x\n",
    "    starts = np.random.randint(low=0, high=x.shape[1]-target_len, size=(x.shape[0])).astype(int)\n",
    "    ends = (target_len + starts).astype(int)\n",
    "    \n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        for dim in range(x.shape[2]):\n",
    "            ret[i,:,dim] = np.interp(np.linspace(0, target_len, num=x.shape[1]), np.arange(target_len), pat[starts[i]:ends[i],dim]).T\n",
    "    return ret\n",
    "\n",
    "def window_warp(x, window_ratio=0.1, scales=[0.5, 2.]):\n",
    "    # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n",
    "    warp_scales = np.random.choice(scales, x.shape[0])\n",
    "    warp_size = np.ceil(window_ratio*x.shape[1]).astype(int)\n",
    "    window_steps = np.arange(warp_size)\n",
    "\n",
    "    window_starts = np.random.randint(low=1, high=x.shape[1]-warp_size-1, size=(x.shape[0])).astype(int)\n",
    "    window_ends = (window_starts + warp_size).astype(int)\n",
    "\n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        for dim in range(x.shape[2]):\n",
    "            start_seg = pat[:window_starts[i],dim]\n",
    "            window_seg = np.interp(np.linspace(0, warp_size-1, num=int(warp_size*warp_scales[i])), window_steps, pat[window_starts[i]:window_ends[i],dim])\n",
    "            end_seg = pat[window_ends[i]:,dim]\n",
    "            warped = np.concatenate((start_seg, window_seg, end_seg))                \n",
    "            ret[i,:,dim] = np.interp(np.arange(x.shape[1]), np.linspace(0, x.shape[1]-1., num=warped.size), warped).T\n",
    "    return ret\n",
    "\n",
    "def magnitude_warp(x, sigma=0.2, knot=4):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    orig_steps = np.arange(x.shape[1])\n",
    "\n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n",
    "    warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n",
    "    ret = np.zeros_like(x)\n",
    "    for i, pat in enumerate(x):\n",
    "        warper = np.array([CubicSpline(warp_steps[:,dim], random_warps[i,:,dim])(orig_steps) for dim in range(x.shape[2])]).T\n",
    "        ret[i] = pat * warper\n",
    "\n",
    "    return ret\n",
    "\n",
    "augmentations = [jitter, scaling, time_warp, window_slice, window_warp, magnitude_warp]\n",
    "\n",
    "def apply_random_aug(x, y):\n",
    "    for i in range(x.shape[0]):\n",
    "        s = np.expand_dims(x[i], axis = 0)\n",
    "        yi = np.expand_dims(y[i], axis = 0)\n",
    "        rng = default_rng()\n",
    "        indexes = rng.choice(len(augmentations), size=2, replace=False)\n",
    "\n",
    "        for index in indexes:\n",
    "            x = np.append(x, augmentations[index](s), axis = 0)\n",
    "            y = np.append(y, yi, axis = 0)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "These functions are used to scale and normalize a 3D NumPy array (i.e., an array of 2D arrays) sample-wise using NumPy library. In particular for this project the considered 3D array is the one containing the needed data, so an array of time series. Here's how they work:\n",
    "\n",
    "- The scalerSampleWise() function takes a 3D NumPy array array as input, where the first dimension represents the time series samples, and the other two dimensions represent the time series (time steps x features). The function then iterates over each sample using a for loop and calculates the maximum and minimum values for each sample using the max() and min() methods, respectively.\n",
    "- The function then normalizes each sample by subtracting the minimum value and dividing by the range (i.e., max-min), which scales the sample between 0 and 1.\n",
    "- The normalizerSampleWise() function takes a 3D NumPy array array as input, where the first dimension represents the samples, and the other two dimensions represent the features. The function then iterates over each sample using a for loop and calculates the mean and standard deviation for each sample using the mean() and std() methods, respectively.\n",
    "- The function then normalizes each sample by subtracting the mean and dividing by the standard deviation, which scales the sample to have a mean of 0 and a standard deviation of 1.\n",
    "- Both functions modify the input array in place and return the modified array as output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__183pn56YqI"
   },
   "outputs": [],
   "source": [
    "def scalerSampleWise(array):\n",
    "    for i in range(array.shape[0]):\n",
    "        max = array[i].max()\n",
    "        min = array[i].min()\n",
    "\n",
    "        array[i] = (array[i]-min)/(max-min)\n",
    "\n",
    "    return array\n",
    "\n",
    "\n",
    "def normalizerSampleWise(array):\n",
    "    for i in range(array.shape[0]):\n",
    "        mean = array[i].mean()\n",
    "        std = array[i].std()\n",
    "\n",
    "        array[i] = (array[i]-mean)/std\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgaVZG07x4-a"
   },
   "outputs": [],
   "source": [
    "x_data_path = \"x_train.npy\"\n",
    "y_data_path = \"y_train.npy\"\n",
    "\n",
    "x_data = np.load(x_data_path)\n",
    "y_data = np.load(y_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stratified Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKXupgjnr7Dd"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2, random_state = seed, stratify = y_data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = seed, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeofifYb2-Yh",
    "outputId": "fcfc7c84-a93e-4393-f5fa-57a3ba2782a1"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1748, 36, 6), (486, 36, 6), (195, 36, 6))"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLea_h9y9svZ"
   },
   "outputs": [],
   "source": [
    "X_train = x_data\n",
    "y_train = y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data augmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9XO4ihx-jp5"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = apply_random_aug(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert time series data to images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fE6IZh6A_Pep"
   },
   "outputs": [],
   "source": [
    "X_train1 = timeSeriesToImage(X_train)\n",
    "X_test1 = timeSeriesToImage(X_test)\n",
    "X_val1 = timeSeriesToImage(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFZ1nPpo_yQD",
    "outputId": "415d1084-ed68-407c-c7ba-e8f1a2d370f5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7287, 36, 36, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalize & Standardize time series data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = scalerSampleWise(X_train)\n",
    "X_test = scalerSampleWise(X_test)\n",
    "X_val = scalerSampleWise(X_val)\n",
    "\n",
    "X_train = normalizerSampleWise(X_train)\n",
    "X_test = normalizerSampleWise(X_test)\n",
    "X_val = normalizerSampleWise(X_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize time series images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (50,50)"
   ],
   "metadata": {
    "id": "mxiKfCWRB8SD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 36 X 6 time series image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.imshow(x_data[1], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('timeseriesImageBefore.png', dpi=100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Plkoima8_Sru",
    "outputId": "dd6e64ab-acb9-435c-cbd9-ed508e901475"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 3600x3600 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 36 X 36 x 3 Time series image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.imshow(X_train1[1]/255)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('timeseriesImageAfter.png', dpi=100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tt-XpGgE_Pw6",
    "outputId": "18489cc0-79ac-4fe7-d085-8e1ac33eb138"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 3600x3600 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8ytNYcN2Hc36",
    "outputId": "a87b3092-9097-43da-cc00-1f945dda272b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_img = 20\n",
    "\n",
    "fig, axes = plt.subplots(5, num_img//5, figsize=(20,num_img))\n",
    "axes = axes.flatten()\n",
    "for i in range(num_img):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(X_test1[i]/255)\n",
    "    ax.title.set_text(\"class: \" + str(y_test[i]))\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensorflow Dataset to handle the 2 types of inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCKwrbY_Eaj1"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SHUFFLE_BUFFER_SIZE = 1000000\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration= True).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "val_dataset= val_dataset.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset1 = tf.data.Dataset.from_tensor_slices(({'input1': X_train1, 'input2': X_train}, y_train))\n",
    "test_dataset1 = tf.data.Dataset.from_tensor_slices(({'input1': X_test1, 'input2': X_test},y_test))\n",
    "val_dataset1 = tf.data.Dataset.from_tensor_slices(({'input1': X_val1, 'input2': X_val},y_val))\n",
    "train_dataset1 = train_dataset1.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration= True).batch(BATCH_SIZE)\n",
    "test_dataset1 = test_dataset1.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "val_dataset1 = val_dataset1.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi9TiYH0nrW9"
   },
   "source": [
    "# Cyclical LR"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cyclical learning rate (CLR) is a technique used in deep learning that involves varying the learning rate of the optimizer during training. Instead of using a fixed learning rate throughout the entire training process, the learning rate is gradually increased and then decreased in a cyclical manner. This technique was introduced by Leslie N. Smith in 2015.\n",
    "\n",
    "The idea behind CLR is to allow the model to explore a wider range of learning rates and find the optimal learning rate for the given problem. This is achieved by gradually increasing the learning rate from a lower bound to an upper bound and then decreasing it back to the lower bound. This cycle can be repeated multiple times during the training process.\n",
    "\n",
    "The benefits of CLR include faster convergence to the optimal solution and better generalization performance of the model. It can also help prevent the model from getting stuck in local minima by allowing it to escape to a different part of the loss landscape.\n",
    "\n",
    "CLR can be implemented using various techniques, including triangular learning rate policy, triangular2 learning rate policy, and exponential learning rate policy. These policies differ in how the learning rate is varied over each cycle. The choice of policy depends on the specific problem and the architecture of the model being trained."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cd9HkdlweEg4"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convnext, vision-based"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJ86Dmtk6Tyu"
   },
   "outputs": [],
   "source": [
    "input_shape = (36, 36, 3)\n",
    "classes = y_train.shape[-1]\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwcugUvAQw1h"
   },
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', classes = np.unique(y_train),y = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Supernet instantiation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54c3kuZW5ht-",
    "outputId": "ca8e403a-bc30-4c05-e0fb-7c1c107b2d54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295\n",
      "Model: \"convnext_large\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " convnext_large_prestem_normali  (None, 224, 224, 3)  0          ['input_3[0][0]']                \n",
      " zation (Normalization)                                                                           \n",
      "                                                                                                  \n",
      " convnext_large_stem (Sequentia  (None, 56, 56, 192)  9792       ['convnext_large_prestem_normaliz\n",
      " l)                                                              ation[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_0  (None, 56, 56, 192)  9600       ['convnext_large_stem[0][0]']    \n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_0  (None, 56, 56, 192)  384        ['convnext_large_stage_0_block_0_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_0  (None, 56, 56, 768)  148224     ['convnext_large_stage_0_block_0_\n",
      " _pointwise_conv_1 (Dense)                                       layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_0  (None, 56, 56, 768)  0          ['convnext_large_stage_0_block_0_\n",
      " _gelu (Activation)                                              pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_0  (None, 56, 56, 192)  147648     ['convnext_large_stage_0_block_0_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_0  (None, 56, 56, 192)  192        ['convnext_large_stage_0_block_0_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_0  (None, 56, 56, 192)  0          ['convnext_large_stage_0_block_0_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_72 (TFOpL  (None, 56, 56, 192)  0          ['convnext_large_stem[0][0]',    \n",
      " ambda)                                                           'convnext_large_stage_0_block_0_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_1  (None, 56, 56, 192)  9600       ['tf.__operators__.add_72[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_1  (None, 56, 56, 192)  384        ['convnext_large_stage_0_block_1_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_1  (None, 56, 56, 768)  148224     ['convnext_large_stage_0_block_1_\n",
      " _pointwise_conv_1 (Dense)                                       layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_1  (None, 56, 56, 768)  0          ['convnext_large_stage_0_block_1_\n",
      " _gelu (Activation)                                              pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_1  (None, 56, 56, 192)  147648     ['convnext_large_stage_0_block_1_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_1  (None, 56, 56, 192)  192        ['convnext_large_stage_0_block_1_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_1  (None, 56, 56, 192)  0          ['convnext_large_stage_0_block_1_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_73 (TFOpL  (None, 56, 56, 192)  0          ['tf.__operators__.add_72[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_0_block_1_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_2  (None, 56, 56, 192)  9600       ['tf.__operators__.add_73[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_2  (None, 56, 56, 192)  384        ['convnext_large_stage_0_block_2_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_2  (None, 56, 56, 768)  148224     ['convnext_large_stage_0_block_2_\n",
      " _pointwise_conv_1 (Dense)                                       layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_2  (None, 56, 56, 768)  0          ['convnext_large_stage_0_block_2_\n",
      " _gelu (Activation)                                              pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_2  (None, 56, 56, 192)  147648     ['convnext_large_stage_0_block_2_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_2  (None, 56, 56, 192)  192        ['convnext_large_stage_0_block_2_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_0_block_2  (None, 56, 56, 192)  0          ['convnext_large_stage_0_block_2_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_74 (TFOpL  (None, 56, 56, 192)  0          ['tf.__operators__.add_73[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_0_block_2_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_downsampling_bl  (None, 28, 28, 384)  295680     ['tf.__operators__.add_74[0][0]']\n",
      " ock_0 (Sequential)                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_0  (None, 28, 28, 384)  19200      ['convnext_large_downsampling_blo\n",
      " _depthwise_conv (Conv2D)                                        ck_0[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_0  (None, 28, 28, 384)  768        ['convnext_large_stage_1_block_0_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_0  (None, 28, 28, 1536  591360     ['convnext_large_stage_1_block_0_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_0  (None, 28, 28, 1536  0          ['convnext_large_stage_1_block_0_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_0  (None, 28, 28, 384)  590208     ['convnext_large_stage_1_block_0_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_0  (None, 28, 28, 384)  384        ['convnext_large_stage_1_block_0_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_0  (None, 28, 28, 384)  0          ['convnext_large_stage_1_block_0_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_75 (TFOpL  (None, 28, 28, 384)  0          ['convnext_large_downsampling_blo\n",
      " ambda)                                                          ck_0[0][0]',                     \n",
      "                                                                  'convnext_large_stage_1_block_0_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_1  (None, 28, 28, 384)  19200      ['tf.__operators__.add_75[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_1  (None, 28, 28, 384)  768        ['convnext_large_stage_1_block_1_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_1  (None, 28, 28, 1536  591360     ['convnext_large_stage_1_block_1_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_1  (None, 28, 28, 1536  0          ['convnext_large_stage_1_block_1_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_1  (None, 28, 28, 384)  590208     ['convnext_large_stage_1_block_1_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_1  (None, 28, 28, 384)  384        ['convnext_large_stage_1_block_1_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_1  (None, 28, 28, 384)  0          ['convnext_large_stage_1_block_1_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_76 (TFOpL  (None, 28, 28, 384)  0          ['tf.__operators__.add_75[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_1_block_1_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_2  (None, 28, 28, 384)  19200      ['tf.__operators__.add_76[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_2  (None, 28, 28, 384)  768        ['convnext_large_stage_1_block_2_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_2  (None, 28, 28, 1536  591360     ['convnext_large_stage_1_block_2_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_2  (None, 28, 28, 1536  0          ['convnext_large_stage_1_block_2_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_2  (None, 28, 28, 384)  590208     ['convnext_large_stage_1_block_2_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_2  (None, 28, 28, 384)  384        ['convnext_large_stage_1_block_2_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_1_block_2  (None, 28, 28, 384)  0          ['convnext_large_stage_1_block_2_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_77 (TFOpL  (None, 28, 28, 384)  0          ['tf.__operators__.add_76[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_1_block_2_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_downsampling_bl  (None, 14, 14, 768)  1181184    ['tf.__operators__.add_77[0][0]']\n",
      " ock_1 (Sequential)                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_0  (None, 14, 14, 768)  38400      ['convnext_large_downsampling_blo\n",
      " _depthwise_conv (Conv2D)                                        ck_1[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_0  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_0_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_0  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_0_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_0  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_0_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_0  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_0_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_0  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_0_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_0  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_0_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_78 (TFOpL  (None, 14, 14, 768)  0          ['convnext_large_downsampling_blo\n",
      " ambda)                                                          ck_1[0][0]',                     \n",
      "                                                                  'convnext_large_stage_2_block_0_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_78[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_1_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_1_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_1_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_1_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_1_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_1_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_79 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_78[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_1_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  38400      ['tf.__operators__.add_79[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_2_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_2_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_2_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_2_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_2_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_2_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_80 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_79[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_2_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_3  (None, 14, 14, 768)  38400      ['tf.__operators__.add_80[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_3  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_3_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_3  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_3_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_3  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_3_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_3  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_3_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_3  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_3_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_3  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_3_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_81 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_80[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_3_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_4  (None, 14, 14, 768)  38400      ['tf.__operators__.add_81[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_4  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_4_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_4  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_4_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_4  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_4_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_4  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_4_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_4  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_4_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_4  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_4_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_82 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_81[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_4_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_5  (None, 14, 14, 768)  38400      ['tf.__operators__.add_82[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_5  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_5_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_5  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_5_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_5  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_5_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_5  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_5_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_5  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_5_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_5  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_5_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_83 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_82[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_5_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_6  (None, 14, 14, 768)  38400      ['tf.__operators__.add_83[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_6  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_6_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_6  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_6_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_6  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_6_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_6  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_6_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_6  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_6_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_6  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_6_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_84 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_83[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_6_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_7  (None, 14, 14, 768)  38400      ['tf.__operators__.add_84[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_7  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_7_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_7  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_7_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_7  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_7_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_7  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_7_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_7  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_7_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_7  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_7_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_85 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_84[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_7_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_8  (None, 14, 14, 768)  38400      ['tf.__operators__.add_85[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_8  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_8_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_8  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_8_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_8  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_8_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_8  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_8_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_8  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_8_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_8  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_8_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_86 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_85[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_8_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_9  (None, 14, 14, 768)  38400      ['tf.__operators__.add_86[0][0]']\n",
      " _depthwise_conv (Conv2D)                                                                         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_9  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_9_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_9  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_9_\n",
      " _pointwise_conv_1 (Dense)      )                                layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_9  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_9_\n",
      " _gelu (Activation)             )                                pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_9  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_9_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_9  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_9_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_9  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_9_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_87 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_86[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_9_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_87[0][0]']\n",
      " 0_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_10\n",
      " 0_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_10\n",
      " 0_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_10\n",
      " 0_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_10\n",
      " 0_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_10\n",
      " 0_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_10\n",
      " 0_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_88 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_87[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_10\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_88[0][0]']\n",
      " 1_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_11\n",
      " 1_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_11\n",
      " 1_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_11\n",
      " 1_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_11\n",
      " 1_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_11\n",
      " 1_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_11\n",
      " 1_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_89 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_88[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_11\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_89[0][0]']\n",
      " 2_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_12\n",
      " 2_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_12\n",
      " 2_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_12\n",
      " 2_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_12\n",
      " 2_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_12\n",
      " 2_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_12\n",
      " 2_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_90 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_89[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_12\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_90[0][0]']\n",
      " 3_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_13\n",
      " 3_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_13\n",
      " 3_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_13\n",
      " 3_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_13\n",
      " 3_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_13\n",
      " 3_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_13\n",
      " 3_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_91 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_90[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_13\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_91[0][0]']\n",
      " 4_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_14\n",
      " 4_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_14\n",
      " 4_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_14\n",
      " 4_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_14\n",
      " 4_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_14\n",
      " 4_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_14\n",
      " 4_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_92 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_91[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_14\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_92[0][0]']\n",
      " 5_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_15\n",
      " 5_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_15\n",
      " 5_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_15\n",
      " 5_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_15\n",
      " 5_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_15\n",
      " 5_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_15\n",
      " 5_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_93 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_92[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_15\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_93[0][0]']\n",
      " 6_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_16\n",
      " 6_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_16\n",
      " 6_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_16\n",
      " 6_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_16\n",
      " 6_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_16\n",
      " 6_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_16\n",
      " 6_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_94 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_93[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_16\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_94[0][0]']\n",
      " 7_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_17\n",
      " 7_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_17\n",
      " 7_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_17\n",
      " 7_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_17\n",
      " 7_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_17\n",
      " 7_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_17\n",
      " 7_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_95 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_94[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_17\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_95[0][0]']\n",
      " 8_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_18\n",
      " 8_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_18\n",
      " 8_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_18\n",
      " 8_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_18\n",
      " 8_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_18\n",
      " 8_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_18\n",
      " 8_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_96 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_95[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_18\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  38400      ['tf.__operators__.add_96[0][0]']\n",
      " 9_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_19\n",
      " 9_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_19\n",
      " 9_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_19\n",
      " 9_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_19\n",
      " 9_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_19\n",
      " 9_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_1  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_19\n",
      " 9_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_97 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_96[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_19\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  38400      ['tf.__operators__.add_97[0][0]']\n",
      " 0_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_20\n",
      " 0_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_20\n",
      " 0_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_20\n",
      " 0_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_20\n",
      " 0_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_20\n",
      " 0_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_20\n",
      " 0_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_98 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_97[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_20\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  38400      ['tf.__operators__.add_98[0][0]']\n",
      " 1_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_21\n",
      " 1_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_21\n",
      " 1_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_21\n",
      " 1_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_21\n",
      " 1_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_21\n",
      " 1_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_21\n",
      " 1_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_99 (TFOpL  (None, 14, 14, 768)  0          ['tf.__operators__.add_98[0][0]',\n",
      " ambda)                                                           'convnext_large_stage_2_block_21\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  38400      ['tf.__operators__.add_99[0][0]']\n",
      " 2_depthwise_conv (Conv2D)                                                                        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_22\n",
      " 2_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_22\n",
      " 2_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_22\n",
      " 2_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_22\n",
      " 2_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_22\n",
      " 2_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_22\n",
      " 2_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_100 (TFOp  (None, 14, 14, 768)  0          ['tf.__operators__.add_99[0][0]',\n",
      " Lambda)                                                          'convnext_large_stage_2_block_22\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  38400      ['tf.__operators__.add_100[0][0]'\n",
      " 3_depthwise_conv (Conv2D)                                       ]                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_23\n",
      " 3_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_23\n",
      " 3_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_23\n",
      " 3_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_23\n",
      " 3_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_23\n",
      " 3_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_23\n",
      " 3_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_101 (TFOp  (None, 14, 14, 768)  0          ['tf.__operators__.add_100[0][0]'\n",
      " Lambda)                                                         , 'convnext_large_stage_2_block_2\n",
      "                                                                 3_identity[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  38400      ['tf.__operators__.add_101[0][0]'\n",
      " 4_depthwise_conv (Conv2D)                                       ]                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_24\n",
      " 4_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_24\n",
      " 4_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_24\n",
      " 4_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_24\n",
      " 4_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_24\n",
      " 4_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_24\n",
      " 4_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_102 (TFOp  (None, 14, 14, 768)  0          ['tf.__operators__.add_101[0][0]'\n",
      " Lambda)                                                         , 'convnext_large_stage_2_block_2\n",
      "                                                                 4_identity[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  38400      ['tf.__operators__.add_102[0][0]'\n",
      " 5_depthwise_conv (Conv2D)                                       ]                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_25\n",
      " 5_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_25\n",
      " 5_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_25\n",
      " 5_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_25\n",
      " 5_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_25\n",
      " 5_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_25\n",
      " 5_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_103 (TFOp  (None, 14, 14, 768)  0          ['tf.__operators__.add_102[0][0]'\n",
      " Lambda)                                                         , 'convnext_large_stage_2_block_2\n",
      "                                                                 5_identity[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  38400      ['tf.__operators__.add_103[0][0]'\n",
      " 6_depthwise_conv (Conv2D)                                       ]                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  1536       ['convnext_large_stage_2_block_26\n",
      " 6_layernorm (LayerNormalizatio                                  _depthwise_conv[0][0]']          \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  2362368    ['convnext_large_stage_2_block_26\n",
      " 6_pointwise_conv_1 (Dense)     )                                _layernorm[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 3072  0          ['convnext_large_stage_2_block_26\n",
      " 6_gelu (Activation)            )                                _pointwise_conv_1[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  2360064    ['convnext_large_stage_2_block_26\n",
      " 6_pointwise_conv_2 (Dense)                                      _gelu[0][0]']                    \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  768        ['convnext_large_stage_2_block_26\n",
      " 6_layer_scale (LayerScale)                                      _pointwise_conv_2[0][0]']        \n",
      "                                                                                                  \n",
      " convnext_large_stage_2_block_2  (None, 14, 14, 768)  0          ['convnext_large_stage_2_block_26\n",
      " 6_identity (Activation)                                         _layer_scale[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_104 (TFOp  (None, 14, 14, 768)  0          ['tf.__operators__.add_103[0][0]'\n",
      " Lambda)                                                         , 'convnext_large_stage_2_block_2\n",
      "                                                                 6_identity[0][0]']               \n",
      "                                                                                                  \n",
      " convnext_large_downsampling_bl  (None, 7, 7, 1536)  4721664     ['tf.__operators__.add_104[0][0]'\n",
      " ock_2 (Sequential)                                              ]                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_0  (None, 7, 7, 1536)  76800       ['convnext_large_downsampling_blo\n",
      " _depthwise_conv (Conv2D)                                        ck_2[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_0  (None, 7, 7, 1536)  3072        ['convnext_large_stage_3_block_0_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_0  (None, 7, 7, 6144)  9443328     ['convnext_large_stage_3_block_0_\n",
      " _pointwise_conv_1 (Dense)                                       layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_0  (None, 7, 7, 6144)  0           ['convnext_large_stage_3_block_0_\n",
      " _gelu (Activation)                                              pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_0  (None, 7, 7, 1536)  9438720     ['convnext_large_stage_3_block_0_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_0  (None, 7, 7, 1536)  1536        ['convnext_large_stage_3_block_0_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_0  (None, 7, 7, 1536)  0           ['convnext_large_stage_3_block_0_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_105 (TFOp  (None, 7, 7, 1536)  0           ['convnext_large_downsampling_blo\n",
      " Lambda)                                                         ck_2[0][0]',                     \n",
      "                                                                  'convnext_large_stage_3_block_0_\n",
      "                                                                 identity[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_1  (None, 7, 7, 1536)  76800       ['tf.__operators__.add_105[0][0]'\n",
      " _depthwise_conv (Conv2D)                                        ]                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_1  (None, 7, 7, 1536)  3072        ['convnext_large_stage_3_block_1_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_1  (None, 7, 7, 6144)  9443328     ['convnext_large_stage_3_block_1_\n",
      " _pointwise_conv_1 (Dense)                                       layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_1  (None, 7, 7, 6144)  0           ['convnext_large_stage_3_block_1_\n",
      " _gelu (Activation)                                              pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_1  (None, 7, 7, 1536)  9438720     ['convnext_large_stage_3_block_1_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_1  (None, 7, 7, 1536)  1536        ['convnext_large_stage_3_block_1_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_1  (None, 7, 7, 1536)  0           ['convnext_large_stage_3_block_1_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_106 (TFOp  (None, 7, 7, 1536)  0           ['tf.__operators__.add_105[0][0]'\n",
      " Lambda)                                                         , 'convnext_large_stage_3_block_1\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_2  (None, 7, 7, 1536)  76800       ['tf.__operators__.add_106[0][0]'\n",
      " _depthwise_conv (Conv2D)                                        ]                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_2  (None, 7, 7, 1536)  3072        ['convnext_large_stage_3_block_2_\n",
      " _layernorm (LayerNormalization                                  depthwise_conv[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_2  (None, 7, 7, 6144)  9443328     ['convnext_large_stage_3_block_2_\n",
      " _pointwise_conv_1 (Dense)                                       layernorm[0][0]']                \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_2  (None, 7, 7, 6144)  0           ['convnext_large_stage_3_block_2_\n",
      " _gelu (Activation)                                              pointwise_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_2  (None, 7, 7, 1536)  9438720     ['convnext_large_stage_3_block_2_\n",
      " _pointwise_conv_2 (Dense)                                       gelu[0][0]']                     \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_2  (None, 7, 7, 1536)  1536        ['convnext_large_stage_3_block_2_\n",
      " _layer_scale (LayerScale)                                       pointwise_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " convnext_large_stage_3_block_2  (None, 7, 7, 1536)  0           ['convnext_large_stage_3_block_2_\n",
      " _identity (Activation)                                          layer_scale[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_107 (TFOp  (None, 7, 7, 1536)  0           ['tf.__operators__.add_106[0][0]'\n",
      " Lambda)                                                         , 'convnext_large_stage_3_block_2\n",
      "                                                                 _identity[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 7, 7, 1536)  3072        ['tf.__operators__.add_107[0][0]'\n",
      " rmalization)                                                    ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 196,230,336\n",
      "Trainable params: 175,890,432\n",
      "Non-trainable params: 20,339,904\n",
      "__________________________________________________________________________________________________\n",
      "296\n"
     ]
    }
   ],
   "source": [
    "supernet1 = tf.keras.applications.convnext.ConvNeXtLarge(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "count = 1\n",
    "print(len(supernet1.layers))\n",
    "for layer in supernet1.layers:\n",
    "    if count < 80:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "    count += 1\n",
    "\n",
    "supernet1.summary()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN Architecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9GAvm1qx8IhK",
    "outputId": "ff118266-a42b-4f8f-d4b6-139f7d632f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input1 (InputLayer)            [(None, 36, 36, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " resizing_2 (Resizing)          (None, 224, 224, 3)  0           ['input1[0][0]']                 \n",
      "                                                                                                  \n",
      " convnext_large (Functional)    (None, 7, 7, 1536)   196230336   ['resizing_2[0][0]']             \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 1536)        0           ['convnext_large[0][0]']         \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 1536)         0           ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256)          393472      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " input2 (InputLayer)            [(None, 36, 6)]      0           []                               \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 12)           3084        ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 196,626,892\n",
      "Trainable params: 176,286,988\n",
      "Non-trainable params: 20,339,904\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "input1 = tfk.Input((36,36,3), name = 'input1')\n",
    "input2 = tfk.Input((36,6), name='input2')\n",
    "\n",
    "# RESIZING\n",
    "x = tfkl.Resizing(224,224,interpolation = \"bicubic\")(input1)\n",
    "x = supernet1(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "convFeatures = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(256, activation='relu', kernel_initializer = tfk.initializers.HeUniform(seed))(convFeatures)\n",
    "outputs = tf.keras.layers.Dense(12, activation='softmax', kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n",
    "\n",
    "\n",
    "model = tfk.Model(inputs=[input1, input2], outputs=outputs, name='model1')\n",
    "model.compile(loss=SparseCategoricalFocalLoss(class_weight = class_weights, gamma=1.5), optimizer=tfk.optimizers.Adam(learning_rate = 1e-4), metrics='accuracy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvg0tuG88L2E"
   },
   "outputs": [],
   "source": [
    "#Train the model\n",
    "training_samples = int(X_train.shape[0])\n",
    "step_size = 4*training_samples // 8\n",
    "\n",
    "clr = CyclicLR(\n",
    "    mode='triangular',\n",
    "    base_lr=1e-6, \n",
    "    max_lr=1e-5,\n",
    "    step_size= step_size)\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset1,\n",
    "    epochs = 5,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True), clr\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN time series-based"
   ],
   "metadata": {
    "id": "LHogpctRBBWV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koMMctfFF4c0"
   },
   "outputs": [],
   "source": [
    "def build_1DCNN_classifier(input_shape, classes):\n",
    "    # Build the neural network layer by layer\n",
    "    input1 = tfk.Input((36,36,3), name = 'input1')\n",
    "    input2 = tfk.Input((36,6), name='input2')\n",
    "\n",
    "    # Feature extractor\n",
    "    cnn = tfkl.Conv1D(512,5,padding='same',activation='relu')(input2)\n",
    "    cnn = tfkl.Conv1D(256,5,padding='same',activation='relu')(cnn)\n",
    "    cnn = tfkl.Conv1D(128,5,padding='same',activation='relu')(cnn)\n",
    "    gap = tfkl.GlobalAveragePooling1D()(cnn)\n",
    "    dropout = tfkl.Dropout(.1, seed=seed)(gap)\n",
    "\n",
    "    # Classifier\n",
    "    classifier = tfkl.Dense(256, activation='relu')(dropout)\n",
    "\n",
    "    output_layer = tfkl.Dense(12, activation='softmax')(classifier)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=[input1, input2], outputs=output_layer, name='model2')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=SparseCategoricalFocalLoss(class_weight = class_weights , gamma=2), optimizer=tfk.optimizers.Adam(1e-3), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74seRoecGDGg"
   },
   "outputs": [],
   "source": [
    "model2 =build_1DCNN_classifier(input_shape, classes)\n",
    "model2.summary()\n",
    "\n",
    "training_samples = int(X_train.shape[0])\n",
    "step_size = 4*training_samples // 8\n",
    "\n",
    "clr = CyclicLR(\n",
    "        mode='triangular',\n",
    "        base_lr=1e-6,\n",
    "    max_lr=1e-5,\n",
    "        step_size= step_size)\n",
    "\n",
    "history = model2.fit(\n",
    "        train_dataset1,\n",
    "        epochs = 10,\n",
    "        validation_data=val_dataset1,\n",
    "        callbacks = [\n",
    "            tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True), clr\n",
    "        ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8hy2VnBYX9U",
    "outputId": "12821deb-f259-4fa8-8de4-780bcdfecb94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26915"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble (Average based) of Vision CNN & Time-series CNN"
   ],
   "metadata": {
    "id": "zLElTBIOBPQf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-asN2gTDIdBw"
   },
   "outputs": [],
   "source": [
    "models = [model, model2]\n",
    "input1 = tfk.Input((36,36,3), name = 'input1')\n",
    "input2 = tfk.Input((36,6), name='input2')\n",
    "model_outputs = [model([input1, input2]) for model in models]\n",
    "ensemble_output = tfkl.Average()(model_outputs)\n",
    "ensemble_model = tfk.Model(inputs=[input1, input2], outputs=ensemble_output, name=\"ciao\")\n",
    "ensemble_model.compile(loss=SparseCategoricalFocalLoss(gamma=2), optimizer=tfk.optimizers.Adam(1e-3), metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qFRa8vnvxHqr"
   },
   "outputs": [],
   "source": [
    "ensemble_model.save(\"models/Ensemble.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing metrics (this was done before training on the whole dataset)"
   ],
   "metadata": {
    "id": "_d5_mnhpB4hf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Standalone models performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPTSDgt6ExF1",
    "outputId": "2655f317-a83c-4f2a-af39-1ce5be00c54f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 2s 77ms/step - loss: 1.8747 - accuracy: 0.7572\n",
      "31/31 [==============================] - 0s 3ms/step - loss: 1.3503 - accuracy: 0.7140\n"
     ]
    }
   ],
   "source": [
    "# Predict the test set \n",
    "predictions1 = model.evaluate(test_dataset1)\n",
    "predictions2 = model2.evaluate(test_dataset1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test-time augmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 6s 119ms/step\n",
      "16/16 [==============================] - 2s 120ms/step\n",
      "16/16 [==============================] - 2s 120ms/step\n",
      "16/16 [==============================] - 2s 118ms/step\n",
      "16/16 [==============================] - 2s 118ms/step\n",
      "16/16 [==============================] - 2s 117ms/step\n",
      "16/16 [==============================] - 2s 117ms/step\n"
     ]
    }
   ],
   "source": [
    "def apply_aug(x, aug):\n",
    "    xaug = np.empty_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "      s = np.expand_dims(x[i], axis = 0)\n",
    "      xaug[i] = aug(s)\n",
    "\n",
    "    return xaug\n",
    "\n",
    "def combine_predictions(predictions):\n",
    "    pred_agg = np.mean(predictions, axis=0)\n",
    "    preds = np.argmax(pred_agg, axis=-1)\n",
    "    return preds\n",
    "\n",
    "def prepare1d(array):\n",
    "  array = scalerSampleWise(array)\n",
    "  return normalizerSampleWise(array)\n",
    "\n",
    "def combinedPred(m, x_test, augmentation=None):\n",
    "\n",
    "    x_test = apply_aug(x_test, augmentation) if augmentation is not None else x_test\n",
    "    x_test_1d = prepare1d(x_test)\n",
    "    x_test_2d = timeSeriesToImage(x_test)\n",
    "    X = (x_test_2d, x_test_1d)\n",
    "\n",
    "    return m.predict(X)\n",
    "\n",
    "def tta_predict(m, x_test):\n",
    "\n",
    "    pred1 = combinedPred(m, x_test)\n",
    "    pred2 = combinedPred(m, x_test, jitter)\n",
    "    pred3 = combinedPred(m, x_test, scaling)\n",
    "    pred4 = combinedPred(m, x_test, time_warp)\n",
    "    pred5 = combinedPred(m, x_test, window_slice)\n",
    "    pred6 = combinedPred(m, x_test, window_warp)\n",
    "    pred7 = combinedPred(m, x_test, magnitude_warp)\n",
    "\n",
    "    out = combine_predictions(np.stack((pred1, pred2, pred3, pred4, pred5, pred6, pred7)))\n",
    "    return out\n",
    "\n",
    "predictions = tta_predict(ensemble_model, X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble model performance (No TTA)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDgEGsNrJy6l",
    "outputId": "7a7677f5-f69f-4660-a7f2-bd734428e541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 8s 77ms/step - loss: 0.5573 - accuracy: 0.7716\n"
     ]
    }
   ],
   "source": [
    "predictions = ensemble_model.evaluate(test_dataset1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble model performance (TTA)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7q7EKfLKjwDs"
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7H5y4odm12Nm",
    "outputId": "11cccc8d-9a62-45a0-89fd-77342cacbd69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7901234567901234"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uttLAvXWE82r"
   },
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n",
    "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm.T, cmap='Blues')\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading model and prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Standalone code to load and perform inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "class model:\n",
    "    def __init__(self, path):\n",
    "        self.model = tf.keras.models.load_model(os.path.join(path, 'Ensemble.h5'),custom_objects={'LayerScale': LayerScale,'Custom>SparseCategoricalFocalLoss': SparseCategoricalFocalLoss})\n",
    "\n",
    "    def jitter(self, x, sigma=0.03):\n",
    "    # https://arxiv.org/pdf/1706.00527.pdf\n",
    "      return x + np.random.normal(loc=0., scale=sigma, size=x.shape)\n",
    "\n",
    "    def scaling(self, x, sigma=0.1):\n",
    "        # https://arxiv.org/pdf/1706.00527.pdf\n",
    "        factor = np.random.normal(loc=1., scale=sigma, size=(x.shape[0],x.shape[2]))\n",
    "        return np.multiply(x, factor[:,np.newaxis,:])\n",
    "\n",
    "\n",
    "    def time_warp(self, x, sigma=0.2, knot=4):\n",
    "        from scipy.interpolate import CubicSpline\n",
    "        orig_steps = np.arange(x.shape[1])\n",
    "\n",
    "        random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n",
    "        warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n",
    "\n",
    "        ret = np.zeros_like(x)\n",
    "        for i, pat in enumerate(x):\n",
    "            for dim in range(x.shape[2]):\n",
    "                time_warp = CubicSpline(warp_steps[:,dim], warp_steps[:,dim] * random_warps[i,:,dim])(orig_steps)\n",
    "                scale = (x.shape[1]-1)/time_warp[-1]\n",
    "                ret[i,:,dim] = np.interp(orig_steps, np.clip(scale*time_warp, 0, x.shape[1]-1), pat[:,dim]).T\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def window_slice(self, x, reduce_ratio=0.9):\n",
    "        # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n",
    "        target_len = np.ceil(reduce_ratio*x.shape[1]).astype(int)\n",
    "        if target_len >= x.shape[1]:\n",
    "            return x\n",
    "        starts = np.random.randint(low=0, high=x.shape[1]-target_len, size=(x.shape[0])).astype(int)\n",
    "        ends = (target_len + starts).astype(int)\n",
    "\n",
    "        ret = np.zeros_like(x)\n",
    "        for i, pat in enumerate(x):\n",
    "            for dim in range(x.shape[2]):\n",
    "                ret[i,:,dim] = np.interp(np.linspace(0, target_len, num=x.shape[1]), np.arange(target_len), pat[starts[i]:ends[i],dim]).T\n",
    "        return ret\n",
    "\n",
    "    def window_warp(self, x, window_ratio=0.1, scales=[0.5, 2.]):\n",
    "        # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n",
    "        warp_scales = np.random.choice(scales, x.shape[0])\n",
    "        warp_size = np.ceil(window_ratio*x.shape[1]).astype(int)\n",
    "        window_steps = np.arange(warp_size)\n",
    "\n",
    "        window_starts = np.random.randint(low=1, high=x.shape[1]-warp_size-1, size=(x.shape[0])).astype(int)\n",
    "        window_ends = (window_starts + warp_size).astype(int)\n",
    "\n",
    "        ret = np.zeros_like(x)\n",
    "        for i, pat in enumerate(x):\n",
    "            for dim in range(x.shape[2]):\n",
    "                start_seg = pat[:window_starts[i],dim]\n",
    "                window_seg = np.interp(np.linspace(0, warp_size-1, num=int(warp_size*warp_scales[i])), window_steps, pat[window_starts[i]:window_ends[i],dim])\n",
    "                end_seg = pat[window_ends[i]:,dim]\n",
    "                warped = np.concatenate((start_seg, window_seg, end_seg))\n",
    "                ret[i,:,dim] = np.interp(np.arange(x.shape[1]), np.linspace(0, x.shape[1]-1., num=warped.size), warped).T\n",
    "        return ret\n",
    "\n",
    "    def magnitude_warp(self, x, sigma=0.2, knot=4):\n",
    "        from scipy.interpolate import CubicSpline\n",
    "        orig_steps = np.arange(x.shape[1])\n",
    "\n",
    "        random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n",
    "        warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n",
    "        ret = np.zeros_like(x)\n",
    "        for i, pat in enumerate(x):\n",
    "            warper = np.array([CubicSpline(warp_steps[:,dim], random_warps[i,:,dim])(orig_steps) for dim in range(x.shape[2])]).T\n",
    "            ret[i] = pat * warper\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def apply_aug(self, x, aug):\n",
    "\n",
    "        xaug = np.empty_like(x)\n",
    "        for i in range(x.shape[0]):\n",
    "          s = np.expand_dims(x[i], axis = 0)\n",
    "          xaug[i] = aug(s)\n",
    "\n",
    "        return xaug\n",
    "\n",
    "    def combine_predictions(self, predictions):\n",
    "        pred_agg = np.mean(predictions, axis=0)\n",
    "        preds = np.argmax(pred_agg, axis=-1)\n",
    "        return preds\n",
    "\n",
    "    def shifter0255(self, array):\n",
    "            for i in range(array.shape[0]):\n",
    "                flattened = array[i].flatten()\n",
    "                shifted = np.interp(flattened, (flattened.min(), flattened.max()), (0, 255))\n",
    "                array[i] = np.reshape(shifted, array[i].shape)\n",
    "    def timeSeriesToImage(self, x):\n",
    "            x = np.repeat(x, repeats = 6, axis = 2)\n",
    "            shifter0255(x)\n",
    "            x = np.stack([x, x, x], axis = -1)\n",
    "            return x\n",
    "\n",
    "\n",
    "    def scalerSampleWise(self, array):\n",
    "      for i in range(array.shape[0]):\n",
    "        max = array[i].max()\n",
    "        min = array[i].min()\n",
    "\n",
    "        array[i] = (array[i]-min)/(max-min)\n",
    "\n",
    "      return array\n",
    "\n",
    "    def normalizerSampleWise(self, array):\n",
    "      for i in range(array.shape[0]):\n",
    "        mean = array[i].mean()\n",
    "        std = array[i].std()\n",
    "\n",
    "        array[i] = (array[i]-mean)/std\n",
    "\n",
    "      return array\n",
    "\n",
    "    def prepare1d(array):\n",
    "      array = scalerSampleWise(array)\n",
    "      return normalizerSampleWise(array)\n",
    "\n",
    "\n",
    "    def combinedPred(self, m, x_test, augmentation=None):\n",
    "\n",
    "        x_test = apply_aug(x_test, augmentation) if augmentation is not None else x_test\n",
    "        x_test_1d = prepare1d(x_test)\n",
    "        x_test_2d = timeSeriesToImage(x_test)\n",
    "\n",
    "        X = (x_test_2d, x_test_1d)\n",
    "\n",
    "        return m.predict(X)\n",
    "\n",
    "    def tta_predict(self, m, x_test):\n",
    "\n",
    "        pred1 = combinedPred(m, x_test)\n",
    "        pred2 = combinedPred(m, x_test, self.jitter)\n",
    "        pred3 = combinedPred(m, x_test, self.scaling)\n",
    "        pred4 = combinedPred(m, x_test, self.time_warp)\n",
    "        pred5 = combinedPred(m, x_test, self.window_slice)\n",
    "        pred6 = combinedPred(m, x_test, self.window_warp)\n",
    "        pred7 = combinedPred(m, x_test, self.magnitude_warp)\n",
    "\n",
    "        out = combine_predictions(np.stack((pred1, pred2, pred3, pred4, pred5, pred6, pred7)))\n",
    "        return out\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        X_np = X.numpy()\n",
    "        out = tta_predict(self.model, X_np)\n",
    "        out = tf.convert_to_tensor(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class LayerScale(tf.keras.layers.Layer):\n",
    "    \"\"\"Layer scale module.\n",
    "    References:\n",
    "      - https://arxiv.org/abs/2103.17239\n",
    "    Args:\n",
    "      init_values (float): Initial value for layer scale. Should be within\n",
    "        [0, 1].\n",
    "      projection_dim (int): Projection dimensionality.\n",
    "    Returns:\n",
    "      Tensor multiplied to the scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.init_values = init_values\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            self.init_values * tf.ones((self.projection_dim,))\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.gamma\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"init_values\": self.init_values,\n",
    "                \"projection_dim\": self.projection_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "_EPSILON = tf.keras.backend.epsilon()\n",
    "\n",
    "def sparse_categorical_focal_loss(y_true, y_pred, gamma, *,\n",
    "                                  class_weight = None,\n",
    "                                  from_logits: bool = False, axis: int = -1\n",
    "                                  ) -> tf.Tensor:\n",
    "\n",
    "    # Process focusing parameter\n",
    "    gamma = tf.convert_to_tensor(gamma, dtype=tf.dtypes.float32)\n",
    "    gamma_rank = gamma.shape.rank\n",
    "    scalar_gamma = gamma_rank == 0\n",
    "\n",
    "    # Process class weight\n",
    "    if class_weight is not None:\n",
    "        class_weight = tf.convert_to_tensor(class_weight,\n",
    "                                            dtype=tf.dtypes.float32)\n",
    "\n",
    "    # Process prediction tensor\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_pred_rank = y_pred.shape.rank\n",
    "    if y_pred_rank is not None:\n",
    "        axis %= y_pred_rank\n",
    "        if axis != y_pred_rank - 1:\n",
    "            # Put channel axis last for sparse_softmax_cross_entropy_with_logits\n",
    "            perm = list(itertools.chain(range(axis),\n",
    "                                        range(axis + 1, y_pred_rank), [axis]))\n",
    "            y_pred = tf.transpose(y_pred, perm=perm)\n",
    "    elif axis != -1:\n",
    "        raise ValueError(\n",
    "            f'Cannot compute sparse categorical focal loss with axis={axis} on '\n",
    "            'a prediction tensor with statically unknown rank.')\n",
    "    y_pred_shape = tf.shape(y_pred)\n",
    "\n",
    "    # Process ground truth tensor\n",
    "    y_true = tf.dtypes.cast(y_true, dtype=tf.dtypes.int64)\n",
    "    y_true_rank = y_true.shape.rank\n",
    "\n",
    "    if y_true_rank is None:\n",
    "        raise NotImplementedError('Sparse categorical focal loss not supported '\n",
    "                                  'for target/label tensors of unknown rank')\n",
    "\n",
    "    reshape_needed = (y_true_rank is not None and y_pred_rank is not None and\n",
    "                      y_pred_rank != y_true_rank + 1)\n",
    "    if reshape_needed:\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        y_pred = tf.reshape(y_pred, [-1, y_pred_shape[-1]])\n",
    "\n",
    "    if from_logits:\n",
    "        logits = y_pred\n",
    "        probs = tf.nn.softmax(y_pred, axis=-1)\n",
    "    else:\n",
    "        probs = y_pred\n",
    "        logits = tf.math.log(tf.clip_by_value(y_pred, _EPSILON, 1 - _EPSILON))\n",
    "\n",
    "    xent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y_true,\n",
    "        logits=logits,\n",
    "    )\n",
    "\n",
    "    y_true_rank = y_true.shape.rank\n",
    "    probs = tf.gather(probs, y_true, axis=-1, batch_dims=y_true_rank)\n",
    "    if not scalar_gamma:\n",
    "        gamma = tf.gather(gamma, y_true, axis=0, batch_dims=y_true_rank)\n",
    "    focal_modulation = (1 - probs) ** gamma\n",
    "    loss = focal_modulation * xent_loss\n",
    "\n",
    "    if class_weight is not None:\n",
    "        class_weight = tf.gather(class_weight, y_true, axis=0,\n",
    "                                 batch_dims=y_true_rank)\n",
    "        loss *= class_weight\n",
    "\n",
    "    if reshape_needed:\n",
    "        loss = tf.reshape(loss, y_pred_shape[:-1])\n",
    "\n",
    "    return loss\n",
    "\n",
    "class SparseCategoricalFocalLoss(tf.keras.losses.Loss):\n",
    "\n",
    "    def __init__(self, gamma, class_weight = None,\n",
    "                 from_logits: bool = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.class_weight = class_weight\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(gamma=self.gamma, class_weight=self.class_weight,\n",
    "                      from_logits=self.from_logits)\n",
    "        return config\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        return sparse_categorical_focal_loss(y_true=y_true, y_pred=y_pred,\n",
    "                                             class_weight=self.class_weight,\n",
    "                                             gamma=self.gamma,\n",
    "                                             from_logits=self.from_logits)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pI9bpEm3PFa"
   },
   "outputs": [],
   "source": [
    "m = model(path = \"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJ2atW7j3Vsv"
   },
   "outputs": [],
   "source": [
    "X_test = \"\" # test data\n",
    "preds = m.predict(tf.convert_to_tensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Njx5IpTp3odk"
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LD0ZRsy4aNd"
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
